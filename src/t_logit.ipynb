{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBxKesHgGkOi4ACQo7tiLG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takakishi/HEC_DS_ML_project/blob/main/src/t_logit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Import Data"
      ],
      "metadata": {
        "id": "ljidAy_KXG_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download fr_core_news_sm\n",
        "!pip install textstat\n",
        "# BERT\n",
        "# !pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn9SHRIBjoHz",
        "outputId": "1e9c9773-a642-4439-d0ae-245353f81fbf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UZAiGgP64W9z"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Training and further analysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stopwords\n",
        "# BERT\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "# import torch\n",
        "# from transformers import Trainer, TrainingArguments\n",
        "# GridSearchCV for hyperparameter tuning of the logistic regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Check if SVM model performs better\n",
        "from sklearn.svm import SVC\n",
        "# Further\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack\n",
        "import textstat\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Data\n",
        "sample_submission = pd.read_csv('https://raw.githubusercontent.com/takakishi/HEC_DS_ML_project/main/data/data_raw/sample_submission.csv')\n",
        "training_data = pd.read_csv('https://raw.githubusercontent.com/takakishi/HEC_DS_ML_project/main/data/data_raw/training_data.csv')\n",
        "unlabelled_test_data = pd.read_csv('https://raw.githubusercontent.com/takakishi/HEC_DS_ML_project/main/data/data_raw/unlabelled_test_data.csv')"
      ],
      "metadata": {
        "id": "MvntlSaz5xoF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMCxRGu16gai",
        "outputId": "512eb3a8-e466-4b34-cfc4-42d63df5acca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of         id difficulty\n",
              "0        0         A1\n",
              "1        1         A1\n",
              "2        2         A1\n",
              "3        3         A1\n",
              "4        4         A1\n",
              "...    ...        ...\n",
              "1195  1195         A1\n",
              "1196  1196         A1\n",
              "1197  1197         A1\n",
              "1198  1198         A1\n",
              "1199  1199         A1\n",
              "\n",
              "[1200 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gz6h5Rp68pl",
        "outputId": "9962b209-46da-44b8-ff13-2ffb2bd1298e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of         id                                           sentence difficulty\n",
              "0        0  Les coûts kilométriques réels peuvent diverger...         C1\n",
              "1        1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
              "2        2  Le test de niveau en français est sur le site ...         A1\n",
              "3        3           Est-ce que ton mari est aussi de Boston?         A1\n",
              "4        4  Dans les écoles de commerce, dans les couloirs...         B1\n",
              "...    ...                                                ...        ...\n",
              "4795  4795  C'est pourquoi, il décida de remplacer les hab...         B2\n",
              "4796  4796  Il avait une de ces pâleurs splendides qui don...         C1\n",
              "4797  4797  Et le premier samedi de chaque mois, venez ren...         A2\n",
              "4798  4798  Les coûts liés à la journalisation n'étant pas...         C2\n",
              "4799  4799  Sur le sable, la mer haletait de toute la resp...         C2\n",
              "\n",
              "[4800 rows x 3 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_test_data.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNVPopt86-Dh",
        "outputId": "f2786dcc-b50a-4a97-f6c3-d2f9595199ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of         id                                           sentence\n",
              "0        0  Nous dûmes nous excuser des propos que nous eû...\n",
              "1        1  Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
              "2        2  Et, paradoxalement, boire froid n'est pas la b...\n",
              "3        3  Ce n'est pas étonnant, car c'est une saison my...\n",
              "4        4  Le corps de Golo lui-même, d'une essence aussi...\n",
              "...    ...                                                ...\n",
              "1195  1195  C'est un phénomène qui trouve une accélération...\n",
              "1196  1196  Je vais parler au serveur et voir si on peut d...\n",
              "1197  1197  Il n'était pas comme tant de gens qui par pare...\n",
              "1198  1198      Ils deviennent dangereux pour notre économie.\n",
              "1199  1199  Son succès a généré beaucoup de réactions néga...\n",
              "\n",
              "[1200 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions and Data Splitting"
      ],
      "metadata": {
        "id": "uWbvttOrpzjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Basic Preprocessing (Logit: 0.44, RF: 0.383)\n",
        "\n",
        "def preprocess_basic(text):\n",
        "    text = text.lower()  # Convert to lower case\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Apply Basic Preprocessing\n",
        "training_data['processed_sentence_basic'] = training_data['sentence'].apply(preprocess_basic)\n",
        "\n",
        "# Initialize and Fit TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_basic = tfidf_vectorizer.fit_transform(training_data['processed_sentence_basic'])\n",
        "y_basic = training_data['difficulty']\n",
        "\n",
        "# Split Basic Preprocessed Data\n",
        "X_train_basic, X_val_basic, y_train_basic, y_val_basic = train_test_split(X_basic, y_basic, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9OVhrnXbXL6v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Tokenization and Stopword Removal (0.405, RF: 0.362)\n",
        "def preprocess_with_stopwords(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "training_data['processed_sentence_stopwords'] = training_data['sentence'].apply(preprocess_with_stopwords)\n",
        "\n",
        "X_stopwords = tfidf_vectorizer.fit_transform(training_data['processed_sentence_stopwords'])\n",
        "y_stopwords = training_data['difficulty']\n",
        "\n",
        "X_train_stopwords, X_val_stopwords, y_train_stopwords, y_val_stopwords = train_test_split(X_stopwords, y_stopwords, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "gQxzz0Ztszea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Lemmatization and POS Tagging\n",
        "def preprocess_with_lemmatization_pos(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    pos_tags = [token.pos_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens), pos_tags\n",
        "\n",
        "# Apply lemmatization and POS tagging preprocessing\n",
        "training_data['processed_sentence_lemmatization_pos'] = training_data['sentence'].apply(lambda x: preprocess_with_lemmatization_pos(x)[0])\n",
        "\n",
        "# Fit and transform with TF-IDF\n",
        "X_lemmatization_pos = tfidf_vectorizer.fit_transform(training_data['processed_sentence_lemmatization_pos'])\n",
        "y_lemmatization_pos = training_data['difficulty']\n",
        "\n",
        "# Split the data\n",
        "X_train_lemmatization_pos, X_val_lemmatization_pos, y_train_lemmatization_pos, y_val_lemmatization_pos = train_test_split(X_lemmatization_pos, y_lemmatization_pos, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tvdRKuzws-Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Advanced Preprocessing with NER and Dependency Parsing\n",
        "def preprocess_advanced(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct or token.ent_type_:\n",
        "            continue\n",
        "        tokens.append(token.lemma_)\n",
        "    ner = [ent.label_ for ent in doc.ents]\n",
        "    dep_parse = [(token.text, token.dep_) for token in doc]\n",
        "    return ' '.join(tokens), ner, dep_parse\n",
        "\n",
        "# Apply advanced preprocessing\n",
        "training_data['processed_sentence_advanced'] = training_data['sentence'].apply(lambda x: preprocess_advanced(x)[0])\n",
        "\n",
        "# Fit and transform with TF-IDF\n",
        "X_advanced = tfidf_vectorizer.fit_transform(training_data['processed_sentence_advanced'])\n",
        "y_advanced = training_data['difficulty']\n",
        "\n",
        "# Split the data\n",
        "X_train_advanced, X_val_advanced, y_train_advanced, y_val_advanced = train_test_split(X_advanced, y_advanced, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "YlbaSrSjtGL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "def preprocess_with_entity_exclusion(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    # Tokenize and apply advanced preprocessing\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        # Skip stopwords and punctuation\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "\n",
        "        # Append lemmatized form of the token if it's not a named entity\n",
        "        if token.ent_type_ == '':\n",
        "            tokens.append(token.lemma_)\n",
        "\n",
        "    # Reconstruct text\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing with entity exclusion\n",
        "training_data['processed_sentence_entity_exclusion'] = training_data['sentence'].apply(preprocess_with_entity_exclusion)\n",
        "\n",
        "# Fit and transform with TF-IDF\n",
        "X_entity_exclusion = tfidf_vectorizer.fit_transform(training_data['processed_sentence_entity_exclusion'])\n",
        "y_entity_exclusion = training_data['difficulty']\n",
        "\n",
        "# Split the data\n",
        "X_train_entity_exclusion, X_val_entity_exclusion, y_train_entity_exclusion, y_val_entity_exclusion = train_test_split(X_entity_exclusion, y_entity_exclusion, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "b7w9wiSYp2i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the data"
      ],
      "metadata": {
        "id": "iXRxAO0MXYJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-OyyKxpXomz",
        "outputId": "9f0d9007-4689-4748-b84f-c5cc24e189b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3840, 14275), (960, 14275))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logit (with Hyperparameter Tuning), etc"
      ],
      "metadata": {
        "id": "rTGCWuIrhz-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Basic Preprocessing\n",
        "\n",
        "# 1-1. Simplest ----\n",
        "log_reg_basic = LogisticRegression(random_state=42)\n",
        "log_reg_basic.fit(X_train_basic, y_train_basic)\n",
        "y_val_pred_basic = log_reg_basic.predict(X_val_basic)\n",
        "accuracy_basic = accuracy_score(y_val_basic, y_val_pred_basic)\n",
        "print(\"Accuracy with Basic Preprocessing:\", accuracy_basic)\n",
        "\n",
        "# Hyperparameter Tuning for Logistic Regression (Best Score: 0.453125) ----\n",
        "# Hyperparameter grid\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "# Logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, solver='liblinear')\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_basic, y_train_basic)\n",
        "# Best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "\n",
        "# 1-2. Support Vector Machine ----\n",
        "svm_model = SVC(kernel='linear')\n",
        "# Train the model\n",
        "svm_model.fit(X_train_basic, y_train_basic)\n",
        "# Evaluate the model\n",
        "y_val_pred_svm = svm_model.predict(X_val_basic)\n",
        "accuracy_svm = accuracy_score(y_val_basic, y_val_pred_svm)\n",
        "print(\"Accuracy with SVM:\", accuracy_svm)\n",
        "\n",
        "\n",
        "# 1-3. Feature Engineering 1 - Adding Sentence Length (0.475) ----\n",
        "training_data['sentence_length'] = training_data['sentence'].apply(lambda x: len(x.split()))\n",
        "scaler = StandardScaler()\n",
        "length_scaled = scaler.fit_transform(training_data[['sentence_length']])\n",
        "\n",
        "# Add sentence length feature to TF-IDF features\n",
        "X_with_length_basic = hstack((X_basic, length_scaled))\n",
        "\n",
        "# Split the data with the new feature\n",
        "X_train_length_basic, X_val_length_basic, y_train_length_basic, y_val_length_basic = train_test_split(X_with_length_basic, y_basic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression with the new feature\n",
        "log_reg_length_basic = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_length_basic.fit(X_train_length_basic, y_train_length_basic)\n",
        "\n",
        "# Evaluate the model\n",
        "y_val_pred_length_basic = log_reg_length_basic.predict(X_val_length_basic)\n",
        "accuracy_length_basic = accuracy_score(y_val_length_basic, y_val_pred_length_basic)\n",
        "print(\"Accuracy with Sentence Length Feature for Basic Preprocessing:\", accuracy_length_basic)\n",
        "\n",
        "\n",
        "# 1-4. Feature Engineering 2 - Calculate readability score (0.4729) ----\n",
        "training_data['readability_score'] = training_data['processed_sentence_basic'].apply(textstat.flesch_reading_ease)\n",
        "readability_scaled = scaler.fit_transform(training_data[['readability_score']])\n",
        "\n",
        "# Combine readability score with TF-IDF and sentence length features\n",
        "X_with_readability = hstack((X_with_length_basic, readability_scaled))\n",
        "\n",
        "# Split the data\n",
        "X_train_readability, X_val_readability, y_train_readability, y_val_readability = train_test_split(X_with_readability, y_basic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate logistic regression\n",
        "log_reg_readability = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_readability.fit(X_train_readability, y_train_readability)\n",
        "y_val_pred_readability = log_reg_readability.predict(X_val_readability)\n",
        "accuracy_readability = accuracy_score(y_val_readability, y_val_pred_readability)\n",
        "print(\"Accuracy with Readability Feature:\", accuracy_readability)\n",
        "\n",
        "\n",
        "# 1-5 Feature Engineering 2 (0.401) ----\n",
        "# Create an imputer object with a strategy of replacing NaN values with the mean of the column\n",
        "# Standardize the additional features\n",
        "scaler = StandardScaler()\n",
        "length_scaled = scaler.fit_transform(training_data[['sentence_length']])\n",
        "readability_scaled = scaler.fit_transform(training_data[['readability_score']])\n",
        "\n",
        "# Combine TF-IDF features with the additional features\n",
        "X_combined = hstack([X_basic, length_scaled, readability_scaled])\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train_more_features, X_val_more_features, y_train_more_features, y_val_more_features = train_test_split(X_combined, y_basic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and apply the imputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train_more_features)\n",
        "X_val_imputed = imputer.transform(X_val_more_features)\n",
        "\n",
        "# Train logistic regression with the imputed feature set\n",
        "log_reg_imputed = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_imputed.fit(X_train_imputed, y_train_more_features)\n",
        "\n",
        "# Evaluate the model\n",
        "y_val_pred_imputed = log_reg_imputed.predict(X_val_imputed)\n",
        "accuracy_imputed = accuracy_score(y_val_more_features, y_val_pred_imputed)\n",
        "print(\"Accuracy with Imputed Feature Set:\", accuracy_imputed)\n",
        "\n",
        "\n",
        "# 1-6. Feature Engineering ----\n",
        "# Reduce the dimensionality of the features\n",
        "svd = TruncatedSVD(n_components=100)  # Adjust n_components based on your dataset\n",
        "X_train_reduced = svd.fit_transform(X_train_more_features)\n",
        "X_val_reduced = svd.transform(X_val_more_features)\n",
        "\n",
        "# Now use the reduced feature set with the HistGradientBoostingClassifier\n",
        "hist_gb_clf = HistGradientBoostingClassifier(random_state=42)\n",
        "hist_gb_clf.fit(X_train_reduced, y_train_more_features)\n",
        "\n",
        "# Evaluate the model\n",
        "y_val_pred_hist_gb = hist_gb_clf.predict(X_val_reduced)\n",
        "accuracy_hist_gb = accuracy_score(y_val_more_features, y_val_pred_hist_gb)\n",
        "print(\"Accuracy with HistGradientBoostingClassifier and Reduced Features:\", accuracy_hist_gb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Y3tSrHxG7C",
        "outputId": "efad1c58-639c-4fd0-85d3-664843a30446"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Basic Preprocessing: 0.440625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best Score: 0.453125\n",
            "Accuracy with SVM: 0.44895833333333335\n",
            "Accuracy with Sentence Length Feature for Basic Preprocessing: 0.475\n",
            "Accuracy with Readability Feature: 0.47291666666666665\n",
            "Accuracy with Imputed Feature Set: 0.47291666666666665\n",
            "Accuracy with HistGradientBoostingClassifier and Reduced Features: 0.415625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Tokenization and Stopword Removal\n",
        "# Logistic Regression for Tokenization and Stopword Removal\n",
        "log_reg_stopwords = LogisticRegression(random_state=42)\n",
        "log_reg_stopwords.fit(X_train_stopwords, y_train_stopwords)\n",
        "y_val_pred_stopwords = log_reg_stopwords.predict(X_val_stopwords)\n",
        "accuracy_stopwords = accuracy_score(y_val_stopwords, y_val_pred_stopwords)\n",
        "print(\"Accuracy with Tokenization and Stopword Removal:\", accuracy_stopwords)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "# Logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, solver='liblinear')\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_stopwords, y_train_stopwords)\n",
        "# Best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY4mxN-zxJ-O",
        "outputId": "7138ace7-4422-49f5-d468-ac9e67cfe663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Tokenization and Stopword Removal: 0.40520833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best Score: 0.39713541666666663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Lemmatization and POS Tagging\n",
        "# Logistic Regression for Lemmatization and POS Tagging\n",
        "log_reg_lemmatization_pos = LogisticRegression(random_state=42)\n",
        "log_reg_lemmatization_pos.fit(X_train_lemmatization_pos, y_train_lemmatization_pos)\n",
        "y_val_pred_lemmatization_pos = log_reg_lemmatization_pos.predict(X_val_lemmatization_pos)\n",
        "accuracy_lemmatization_pos = accuracy_score(y_val_lemmatization_pos, y_val_pred_lemmatization_pos)\n",
        "print(\"Accuracy with Lemmatization and POS Tagging:\", accuracy_lemmatization_pos)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "# Logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, solver='liblinear')\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_lemmatization_pos, y_train_lemmatization_pos)\n",
        "# Best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzh9bR1jxMsh",
        "outputId": "9c5464cc-1618-42cc-fa04-554d9bcc2963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Lemmatization and POS Tagging: 0.3875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best Score: 0.39322916666666663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Advanced Preprocessing with NER and Dependency Parsing\n",
        "# Logistic Regression for Advanced Preprocessing with NER and Dependency Parsing\n",
        "log_reg_advanced = LogisticRegression(random_state=42)\n",
        "log_reg_advanced.fit(X_train_advanced, y_train_advanced)\n",
        "y_val_pred_advanced = log_reg_advanced.predict(X_val_advanced)\n",
        "accuracy_advanced = accuracy_score(y_val_advanced, y_val_pred_advanced)\n",
        "print(\"Accuracy with Advanced Preprocessing (NER & Dependency Parsing):\", accuracy_advanced)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "# Logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, solver='liblinear')\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_advanced, y_train_advanced)\n",
        "# Best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AglLy-NgxQCQ",
        "outputId": "ab83d55b-a1c4-4b07-e5f6-99e9cb9157ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Advanced Preprocessing (NER & Dependency Parsing): 0.38958333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best Score: 0.39010416666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Preprocessing with Entity Exclusion\n",
        "# Logistic Regression for Preprocessing with Entity Exclusion\n",
        "log_reg_entity_exclusion = LogisticRegression(random_state=42)\n",
        "log_reg_entity_exclusion.fit(X_train_entity_exclusion, y_train_entity_exclusion)\n",
        "y_val_pred_entity_exclusion = log_reg_entity_exclusion.predict(X_val_entity_exclusion)\n",
        "accuracy_entity_exclusion = accuracy_score(y_val_entity_exclusion, y_val_pred_entity_exclusion)\n",
        "print(\"Accuracy with Preprocessing with Entity Exclusion:\", accuracy_entity_exclusion)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "# Logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, solver='liblinear')\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_entity_exclusion, y_train_entity_exclusion)\n",
        "# Best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PlCTpB-xVtm",
        "outputId": "896e5da2-6075-440f-f444-60f450a60f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Preprocessing with Entity Exclusion: 0.38958333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best Score: 0.39010416666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "jS7EEzJOh3HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = rf_clf.predict(X_val)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "8D15jNivrVrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "w_wISyURh4i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer and model\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(training_data['difficulty'].unique()))"
      ],
      "metadata": {
        "id": "ImG23P6hh7bZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}